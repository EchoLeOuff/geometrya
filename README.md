# Projet TIPE GeometrIA

## üìñ Pr√©sentation

√Ä travers l'exemple d'un jeu vid√©o (_**Geometry Dash**_), nous allons montrer comment on peut optimiser un mod√®le afin de r√©ussir un niveau.

_**Geometry Dash**_ est une course d‚Äôobstacles. Il n‚Äôy a qu‚Äôune seule commande : **sauter**. Le but est d‚Äôesquiver des **piques** et des **blocs**, puis de finir la course.

Ainsi, nous impl√©menterons un r√©seau de trois neurones :
1. Un qui d√©tecte s‚Äôil y a un **bloc**,

2. Un qui d√©tecte s‚Äôil y a un **pique**,

3. Et le dernier qui **active le saut**.

Ensuite, nous lancerons la premi√®re g√©n√©ration de parties (1 000 parties), o√π le programme va positionner al√©atoirement ses neurones. Parmi ces 1 000 parties, nous garderons celle o√π le personnage est all√© le plus loin dans la course. √Ä partir de celle-ci, nous g√©n√©rerons 1 000 nouvelles parties o√π la position des neurones sera g√©n√©r√©e √† partir de la meilleure partie de la g√©n√©ration pr√©c√©dente.

Une fois le niveau compl√©t√©, nous obtiendrons notre IA ¬´ t√©moin ¬ª, qui se sera **entra√Æn√©e toute seule**.

Enfin, nous g√©n√©rerons d‚Äôautres simulations en ajoutant des conditions initiales sur le positionnement des neurones, dans le but de **r√©duire au maximum le nombre de g√©n√©rations n√©cessaires**.

## üìÇ Architecture du Git


## üîé Bilan du Sprint 1

### üõ† Conception d‚Äôun Niveau Simplifi√©
Nous avons commenc√© par d√©velopper une version simplifi√©e du jeu en Python.
Le but est de cr√©er un environnement contr√¥l√© pour entra√Æner l‚ÄôIA.

‚û°Ô∏è **√Ä ce stade, seules les m√©caniques de base** (saut, d√©placement, gestion des collisions) ont √©t√© impl√©ment√©es.
Un **niveau jouable complet n‚Äôest pas encore disponible**.

### üß† Exploration des Approches Machine Learning
Nous avons explor√© la piste du reinforcement learning avec PyTorch, en √©tudiant notamment deux algorithmes :

- **DQN (Deep Q-Network)** : apprentissage d‚Äôune fonction de valeur Q.

- **PPO (Proximal Policy Optimization)** : plus stable pour l‚Äôentra√Ænement d‚Äôagents RL.

### üë• R√©partition des R√¥les et T√¢ches

|   P√©riode   | T√¢che                               |Responsable |
| ----------- | ----------------------------------- |------------|
| Semaine 1-2 | Recherche sur ML et Pytorch         | Dorian     |
| Semaine 1-2 | Mod√©lisation des m√©caniques de jeux | Vincent    |

### üìö Ressources Consult√©es
- Articles sur **DQN** (Mnih et al., 2015) et **PPO** (Schulman et al., 2017).

- **Documentation officielle PyTorch** (modules torch.nn, torch.optim, etc.).

- Tutoriels ‚ÄúReinforcement Learning‚Äù du site officiel PyTorch.

- Exemples d‚Äôagents IA jouant √† des jeux 2D.


> ‚ö†Ô∏è La documentation est dense et parfois difficile √† aborder au d√©part.
Nous avons pris le temps de clarifier certains concepts cl√©s avant d‚Äôimpl√©menter quoi que ce soit.

### ‚úÖ Solutions Envisag√©es / R√©alis√©es 

| Solution             | Statut            | Description courte                                                |
| -------------------- | ----------------- | ----------------------------------------------------------------- |
| CNN pour d√©tection   | Documentation     | R√©seau de convultion rep√©rage d'obstacle                          |
| Agent DQN en PyTorch | En test           | Agent RL (reinforcement learning) d√©scisionnaire des actions      |
| Simulation Python    | M√©caniques cod√©es | Impl√©mentation des bases du jeu mais pas encore de niveau jouable |

### ‚ö†Ô∏è Difficult√©s et Obstacles

| Difficult√©                               | Solution adopt√©e                                                     |
| ---------------------------------------- | -------------------------------------------------------------------- |
| Compr√©hension des concepts RL et PyTorch | Lecture d‚Äôarticles, tutoriels, documentation officielle              |
| Densit√© de la documentation PyTorch      | N√©cessite du temps pour appropriation ; beaucoup de jargon technique |
| Coordination entre les t√¢ches	           | Mise en place de r√©unions rapides pour synchroniser les avanc√©es     |

### üìå Conclusion et Perspectives

‚úÖ **Avancement actuel :**
- M√©caniques de jeu cod√©es.

- Connaissances th√©oriques solides sur les algorithmes DQN et PPO.

- R√©partition claire des t√¢ches.

üîú **√Ä venir :**
- **Impl√©menter un niveau jouable** pour que l‚Äôagent puisse commencer √† s‚Äôentra√Æner.

- **D√©buter l‚Äôarchitecture du r√©seau de neurones** (DQN).

- **Lancer les premiers tests d‚Äôentra√Ænement IA.**

## üîé Bilan du Sprint 2

### üõ† Conception du premier mod√®le en Q-Learning
Nous avons poursuivie le d√©veloppement de notre version simplifi√©e du jeu en Python.
Puis cr√©ation du premier mod√®le d'apprentissage.

‚û°Ô∏è **√Ä ce stade, le niveau reste incomplet et le premier mod√®le poss√®de encore des axes d'am√©lioration**, le q-learning est fonctionnel.

**Q-learning** : algorithme d'apprentissage par renforcement avec un syst√®me de r√©compense. 

### üß† Exploration des Approches Machine Learning
Nous avons continuer √† √©tudier les diff√©rentes solution de machine learning en python adapt√© √† nos besoins.


### üë• R√©partition des R√¥les et T√¢ches

|   P√©riode   | T√¢che                                                    |Responsable |
| ----------- | ---------------------------------------------------------|------------|
| Semaine 3-4 | approfondissement des recherches sur le machine learning | Vincent    |
| Semaine 3-4 | ajout de l'algorithme d'apprentissage par renforcement   | Dorian     |

### üìö Ressources Consult√©es
- vid√©o youtube de CODE BH - J'ai fait une IA qui apprend √† jouer √† Geometry Dash.
- vid√©o youtube de Thibault Neveu - Apprentissage par renforcement #7 : Deep Q-Learning, apprendre √† conduire
- vid√©o youtube de Siraj Raval - Q Learning Explained (tutorial)
- Exemples d‚Äôagents IA jouant √† des jeux 2D.


### ‚úÖ Solutions Envisag√©es / R√©alis√©es 

| Solution             | Statut            | Description courte                                                 |
| -------------------- | ----------------- | -----------------------------------------------------------------  |
| Agent Q-learning     | Op√©rationnels     | Agent fonctionnelle. Entrainement impossible sans la fin du niveau |
| Simulation Python    | M√©caniques cod√©es | M√©chaniques op√©rationnelles.                                       |
| Niveau               | en cours          | Niveau icomplet qui permet pas encore un apprentissage interessant |

### ‚ö†Ô∏è Difficult√©s et Obstacles

| Difficult√©                               | Solution adopt√©e                                                           |
| ---------------------------------------- | -------------------------------------------------------------------------- |
| Organisation du code                     | on s'est mit d'accord sur les biblioth√®que a utiliser et nettoyage du code |

### üìå Conclusion et Perspectives

‚úÖ **Avancement actuel :**
- M√©caniques de jeu cod√©es et fonctionnelles.

- R√©organisation du code afin d'avoir une meilleur visibilit√©.

- Ajout de l'agent de Q-learning. 



üîú **√Ä venir :**
- **Cr√©er un niveau complet** pour commencer l'entrainement.
- **Lancer les premiers tests d‚Äôentra√Ænement IA.**

## üöÄ Installation rapide

### Pr√©requis
- Python 3.11+ (v√©rifiez avec `python3 --version`)
- Poetry (installez avec `curl -sSL https://install.python-poetry.org | python3 -`)

### √âtapes
1. Clonez le repo :
```bash
git clone https://github.com/tonuser/geometrya.git
cd geometrya
```

2. Installez les d√©pendances (cr√©e l'environnement virtuel automatiquement) :

```bash
poetry install --no-root
```
4. Activez l'environnement :

```bash
eval "$(poetry env activate --shell bash)"  # ou fish: eval (poetry env activate --shell fish)
```
si cela ne fonctionne pas : 

```bash
$ eval $(poetry env activate)
```

5. Lancez vos scripts :

```bash
poetry install --no-root  # Une fois au clean (d√©pendances)
poetry run python train.py # √Ä chaque ex√©cution (training DQN)
```
