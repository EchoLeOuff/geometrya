# Projet TIPE GeometrIA

## üìñ Pr√©sentation

√Ä travers l'exemple d'un jeu vid√©o (_**Geometry Dash**_), nous allons montrer comment on peut optimiser un mod√®le afin de r√©ussir un niveau.

_**Geometry Dash**_ est une course d‚Äôobstacles. Il n‚Äôy a qu‚Äôune seule commande : **sauter**. Le but est d‚Äôesquiver des **piques** et des **blocs**, puis de finir la course.

Ainsi, nous impl√©menterons un r√©seau de trois neurones :
1. Un qui d√©tecte s‚Äôil y a un **bloc**,

2. Un qui d√©tecte s‚Äôil y a un **pique**,

3. Et le dernier qui **active le saut**.

Ensuite, nous lancerons la premi√®re g√©n√©ration de parties (1 000 parties), o√π le programme va positionner al√©atoirement ses neurones. Parmi ces 1 000 parties, nous garderons celle o√π le personnage est all√© le plus loin dans la course. √Ä partir de celle-ci, nous g√©n√©rerons 1 000 nouvelles parties o√π la position des neurones sera g√©n√©r√©e √† partir de la meilleure partie de la g√©n√©ration pr√©c√©dente.

Une fois le niveau compl√©t√©, nous obtiendrons notre IA ¬´ t√©moin ¬ª, qui se sera **entra√Æn√©e toute seule**.

Enfin, nous g√©n√©rerons d‚Äôautres simulations en ajoutant des conditions initiales sur le positionnement des neurones, dans le but de **r√©duire au maximum le nombre de g√©n√©rations n√©cessaires**.

## üìÇ Architecture du Git


## üîé Bilan du Sprint 1

### üõ† Conception d‚Äôun Niveau Simplifi√©
Nous avons commenc√© par d√©velopper une version simplifi√©e du jeu en Python.
Le but est de cr√©er un environnement contr√¥l√© pour entra√Æner l‚ÄôIA.

‚û°Ô∏è **√Ä ce stade, seules les m√©caniques de base** (saut, d√©placement, gestion des collisions) ont √©t√© impl√©ment√©es.
Un **niveau jouable complet n‚Äôest pas encore disponible**.

### üß† Exploration des Approches Machine Learning
Nous avons explor√© la piste du reinforcement learning avec PyTorch, en √©tudiant notamment deux algorithmes :

- **DQN (Deep Q-Network)** : apprentissage d‚Äôune fonction de valeur Q.

- **PPO (Proximal Policy Optimization)** : plus stable pour l‚Äôentra√Ænement d‚Äôagents RL.

### üë• R√©partition des R√¥les et T√¢ches

|   P√©riode   | T√¢che                               |Responsable |
| ----------- | ----------------------------------- |------------|
| Semaine 1-2 | Recherche sur ML et Pytorch         | Dorian     |
| Semaine 1-2 | Mod√©lisation des m√©caniques de jeux | Vincent    |

### üìö Ressources Consult√©es
- Articles sur **DQN** (Mnih et al., 2015) et **PPO** (Schulman et al., 2017).

- **Documentation officielle PyTorch** (modules torch.nn, torch.optim, etc.).

- Tutoriels ‚ÄúReinforcement Learning‚Äù du site officiel PyTorch.

- Exemples d‚Äôagents IA jouant √† des jeux 2D.


> ‚ö†Ô∏è La documentation est dense et parfois difficile √† aborder au d√©part.
Nous avons pris le temps de clarifier certains concepts cl√©s avant d‚Äôimpl√©menter quoi que ce soit.

### ‚úÖ Solutions Envisag√©es / R√©alis√©es 

| Solution             | Statut            | Description courte                                                |
| -------------------- | ----------------- | ----------------------------------------------------------------- |
| CNN pour d√©tection   | Documentation     | R√©seau de convultion rep√©rage d'obstacle                          |
| Agent DQN en PyTorch | En test           | Agent RL (reinforcement learning) d√©scisionnaire des actions      |
| Simulation Python    | M√©caniques cod√©es | Impl√©mentation des bases du jeu mais pas encore de niveau jouable |

### ‚ö†Ô∏è Difficult√©s et Obstacles

| Difficult√©                               | Solution adopt√©e                                                     |
| ---------------------------------------- | -------------------------------------------------------------------- |
| Compr√©hension des concepts RL et PyTorch | Lecture d‚Äôarticles, tutoriels, documentation officielle              |
| Densit√© de la documentation PyTorch      | N√©cessite du temps pour appropriation ; beaucoup de jargon technique |
| Coordination entre les t√¢ches	           | Mise en place de r√©unions rapides pour synchroniser les avanc√©es     |

### üìå Conclusion et Perspectives

‚úÖ **Avancement actuel :**
- M√©caniques de jeu cod√©es.

- Connaissances th√©oriques solides sur les algorithmes DQN et PPO.

- R√©partition claire des t√¢ches.

üîú **√Ä venir :**
- **Impl√©menter un niveau jouable** pour que l‚Äôagent puisse commencer √† s‚Äôentra√Æner.

- **D√©buter l‚Äôarchitecture du r√©seau de neurones** (DQN).

- **Lancer les premiers tests d‚Äôentra√Ænement IA.**

## üìÅ Add your files

- [ ] [Create](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#create-a-file) or [upload](https://docs.gitlab.com/ee/user/project/repository/web_editor.html#upload-a-file) files
- [ ] [Add files using the command line](https://docs.gitlab.com/topics/git/add_files/#add-files-to-a-git-repository) or push an existing Git repository with the following command:

```
cd existing_repo
git remote add origin https://gitlab.com/tipe3/geometry.git
git branch -M main
git push -uf origin main
```

## üîß Integrate with your tools

- [ ] [Set up project integrations](https://gitlab.com/tipe3/geometry/-/settings/integrations)
